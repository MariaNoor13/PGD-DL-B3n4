{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1698504132450,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"asNwnEpouNMp"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"jL5MHk1VuNMs"},"source":["## Training a convnet from scratch on a small dataset"]},{"cell_type":"markdown","metadata":{"id":"L7gbZmtduNMt"},"source":["### The relevance of deep learning for small-data problems"]},{"cell_type":"markdown","metadata":{"id":"3uW5IT52uNMu"},"source":["### Downloading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"ETjNI2AmuNMu"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-4858a0ab-619e-4c18-848a-2c2bd1afefe3\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-4858a0ab-619e-4c18-848a-2c2bd1afefe3\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-9db30b74172f\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# you will chose file for your kaggle.json, downloaded from laggle/settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 163\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["from google.colab import files\n","files.upload()    # you will chose file for your kaggle.json, downloaded from laggle/settings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YeZt5mRauNMu"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"24Nh98bfuNMv"},"outputs":[],"source":["!kaggle competitions download -c dogs-vs-cats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ik5Fkqt_uNMv"},"outputs":[],"source":["!unzip -qq train.zip"]},{"cell_type":"markdown","metadata":{"id":"IT6oXoC0uNMv"},"source":["**Copying images to training, validation, and test directories**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FvGSxTvJuNMv"},"outputs":[],"source":["import os, shutil, pathlib\n","\n","original_dir = pathlib.Path(\"train\")\n","new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n","\n","def make_subset(subset_name, start_index, end_index):\n","    for category in (\"cat\", \"dog\"):\n","        dir = new_base_dir / subset_name / category\n","        os.makedirs(dir)\n","        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n","        for fname in fnames:\n","            shutil.copyfile(src=original_dir / fname,\n","                            dst=dir / fname)\n","\n","make_subset(\"train\", start_index=0, end_index=1000)\n","make_subset(\"validation\", start_index=1000, end_index=1500)\n","make_subset(\"test\", start_index=1500, end_index=2500)"]},{"cell_type":"markdown","metadata":{"id":"xd9UZrGkuNMw"},"source":["### Building the model"]},{"cell_type":"markdown","metadata":{"id":"DB-k0H-ouNMw"},"source":["**Instantiating a small convnet for dogs vs. cats classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2IcvlfNguNMw"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","inputs = keras.Input(shape=(180, 180, 3))\n","x = layers.Rescaling(1./255)(inputs)\n","x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.Flatten()(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lB1nG5stuNMw"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"UhYg4J6KuNMx"},"source":["**Configuring the model for training**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sd4is_JJuNMx"},"outputs":[],"source":["model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"rmsprop\",\n","              metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"prhchJARuNMx"},"source":["### Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WaMj8kTTuNMx"},"source":["**Using `image_dataset_from_directory` to read images**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IPm53tPbuNMx"},"outputs":[],"source":["from tensorflow.keras.utils import image_dataset_from_directory\n","\n","train_dataset = image_dataset_from_directory(\n","    new_base_dir / \"train\",\n","    image_size=(180, 180),\n","    batch_size=32)\n","validation_dataset = image_dataset_from_directory(\n","    new_base_dir / \"validation\",\n","    image_size=(180, 180),\n","    batch_size=32)\n","test_dataset = image_dataset_from_directory(\n","    new_base_dir / \"test\",\n","    image_size=(180, 180),\n","    batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GQ9V99H6uNMx"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","random_numbers = np.random.normal(size=(1000, 16))\n","dataset = tf.data.Dataset.from_tensor_slices(random_numbers)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2XXbVoFVuNMx"},"outputs":[],"source":["for i, element in enumerate(dataset):\n","    print(element.shape)\n","    if i \u003e= 2:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cCvtykZ4uNMy"},"outputs":[],"source":["batched_dataset = dataset.batch(32)\n","for i, element in enumerate(batched_dataset):\n","    print(element.shape)\n","    if i \u003e= 2:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ldxylBw5uNMy"},"outputs":[],"source":["reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n","for i, element in enumerate(reshaped_dataset):\n","    print(element.shape)\n","    if i \u003e= 2:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"vlVf1k7cuNMy"},"source":["**Displaying the shapes of the data and labels yielded by the `Dataset`**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DkcKK3CEuNMy"},"outputs":[],"source":["for data_batch, labels_batch in train_dataset:\n","    print(\"data batch shape:\", data_batch.shape)\n","    print(\"labels batch shape:\", labels_batch.shape)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"VAXStJ6OuNMy"},"source":["**Fitting the model using a `Dataset`**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PtzcTnL9uNMy"},"outputs":[],"source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"convnet_from_scratch.keras\",\n","        save_best_only=True,\n","        monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_dataset,\n","    epochs=30,\n","    validation_data=validation_dataset,\n","    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"1RxYdmH4uNMy"},"source":["**Displaying curves of loss and accuracy during training**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UKOtKK63uNMy"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","accuracy = history.history[\"accuracy\"]\n","val_accuracy = history.history[\"val_accuracy\"]\n","loss = history.history[\"loss\"]\n","val_loss = history.history[\"val_loss\"]\n","epochs = range(1, len(accuracy) + 1)\n","plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n","plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n","plt.title(\"Training and validation accuracy\")\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n","plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n","plt.title(\"Training and validation loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lh7h_Z3xuNMy"},"source":["**Evaluating the model on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zT3tjK-0uNMz"},"outputs":[],"source":["test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"CGNxMVwKuNMz"},"source":["### Using data augmentation"]},{"cell_type":"markdown","metadata":{"id":"bleW9WGVuNMz"},"source":["**Define a data augmentation stage to add to an image model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mAgdx2bduNMz"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(0.1),\n","        layers.RandomZoom(0.2),\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"RUExIcXiuNMz"},"source":["**Displaying some randomly augmented training images**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9bbKkR-puNMz"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","for images, _ in train_dataset.take(1):\n","    for i in range(9):\n","        augmented_images = data_augmentation(images)\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"xz9HW_4uuNMz"},"source":["**Defining a new convnet that includes image augmentation and dropout**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cEGDgCLNuNMz"},"outputs":[],"source":["inputs = keras.Input(shape=(180, 180, 3))\n","x = data_augmentation(inputs)\n","x = layers.Rescaling(1./255)(x)\n","x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.Flatten()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"rmsprop\",\n","              metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"1bug0Ck1uNM0"},"source":["**Training the regularized convnet**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tXNvpsC9uNM3"},"outputs":[],"source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n","        save_best_only=True,\n","        monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_dataset,\n","    epochs=100,\n","    validation_data=validation_dataset,\n","    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"nSxqzz1PuNM3"},"source":["**Evaluating the model on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LFZsImRJuNM3"},"outputs":[],"source":["test_model = keras.models.load_model(\n","    \"convnet_from_scratch_with_augmentation.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"0T1P1GsuuNM3"},"source":["## Leveraging a pretrained model"]},{"cell_type":"markdown","metadata":{"id":"uRgbubs5uNM4"},"source":["### Feature extraction with a pretrained model"]},{"cell_type":"markdown","metadata":{"id":"LD2FsWlduNM4"},"source":["**Instantiating the VGG16 convolutional base**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fvONP_TpuNM4"},"outputs":[],"source":["conv_base = keras.applications.vgg16.VGG16(\n","    weights=\"imagenet\",\n","    include_top=False,\n","    input_shape=(180, 180, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"spDFwWaSuNM4"},"outputs":[],"source":["conv_base.summary()"]},{"cell_type":"markdown","metadata":{"id":"xwpDfjhxuNM4"},"source":["#### Fast feature extraction without data augmentation"]},{"cell_type":"markdown","metadata":{"id":"ZzfCE5hvuNM4"},"source":["**Extracting the VGG16 features and corresponding labels**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tmS37ygPuNM4"},"outputs":[],"source":["import numpy as np\n","\n","def get_features_and_labels(dataset):\n","    all_features = []\n","    all_labels = []\n","    for images, labels in dataset:\n","        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n","        features = conv_base.predict(preprocessed_images)\n","        all_features.append(features)\n","        all_labels.append(labels)\n","    return np.concatenate(all_features), np.concatenate(all_labels)\n","\n","train_features, train_labels =  get_features_and_labels(train_dataset)\n","val_features, val_labels =  get_features_and_labels(validation_dataset)\n","test_features, test_labels =  get_features_and_labels(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZUtIrkkuNM4"},"outputs":[],"source":["train_features.shape"]},{"cell_type":"markdown","metadata":{"id":"ZZlqY-RouNM5"},"source":["**Defining and training the densely connected classifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONA6SLxIuNM5"},"outputs":[],"source":["inputs = keras.Input(shape=(5, 5, 512))\n","x = layers.Flatten()(inputs)\n","x = layers.Dense(256)(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"rmsprop\",\n","              metrics=[\"accuracy\"])\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","      filepath=\"feature_extraction.keras\",\n","      save_best_only=True,\n","      monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_features, train_labels,\n","    epochs=20,\n","    validation_data=(val_features, val_labels),\n","    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"xD6qntTGuNM5"},"source":["**Plotting the results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuo6oOTWuNM5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","acc = history.history[\"accuracy\"]\n","val_acc = history.history[\"val_accuracy\"]\n","loss = history.history[\"loss\"]\n","val_loss = history.history[\"val_loss\"]\n","epochs = range(1, len(acc) + 1)\n","plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n","plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n","plt.title(\"Training and validation accuracy\")\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n","plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n","plt.title(\"Training and validation loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xgQ4jzxSuNM5"},"source":["#### Feature extraction together with data augmentation"]},{"cell_type":"markdown","metadata":{"id":"RMdKff4PuNM5"},"source":["**Instantiating and freezing the VGG16 convolutional base**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwY164k_uNM5"},"outputs":[],"source":["conv_base  = keras.applications.vgg16.VGG16(\n","    weights=\"imagenet\",\n","    include_top=False)\n","conv_base.trainable = False"]},{"cell_type":"markdown","metadata":{"id":"LO-wAvGhuNM6"},"source":["**Printing the list of trainable weights before and after freezing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyCCNqbEuNM6"},"outputs":[],"source":["conv_base.trainable = True\n","print(\"This is the number of trainable weights \"\n","      \"before freezing the conv base:\", len(conv_base.trainable_weights))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3uKLZ-bDuNM6"},"outputs":[],"source":["conv_base.trainable = False\n","print(\"This is the number of trainable weights \"\n","      \"after freezing the conv base:\", len(conv_base.trainable_weights))"]},{"cell_type":"markdown","metadata":{"id":"a-GRgcgSuNM6"},"source":["**Adding a data augmentation stage and a classifier to the convolutional base**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJf3tZ-KuNM6"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(0.1),\n","        layers.RandomZoom(0.2),\n","    ]\n",")\n","\n","inputs = keras.Input(shape=(180, 180, 3))\n","x = data_augmentation(inputs)\n","x = keras.applications.vgg16.preprocess_input(x)\n","x = conv_base(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(256)(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"rmsprop\",\n","              metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNz7QEP8uNM6"},"outputs":[],"source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"feature_extraction_with_data_augmentation.keras\",\n","        save_best_only=True,\n","        monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_dataset,\n","    epochs=50,\n","    validation_data=validation_dataset,\n","    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"IARfZVbWuNM6"},"source":["**Evaluating the model on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mbtkt1VyuNM7"},"outputs":[],"source":["test_model = keras.models.load_model(\n","    \"feature_extraction_with_data_augmentation.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"N9UZrVx0uNM7"},"source":["### Fine-tuning a pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NjT1szSuNM7"},"outputs":[],"source":["conv_base.summary()"]},{"cell_type":"markdown","metadata":{"id":"U-eFRKOiuNM7"},"source":["**Freezing all layers until the fourth from the last**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88PSRLBYuNM7"},"outputs":[],"source":["conv_base.trainable = True\n","for layer in conv_base.layers[:-4]:\n","    layer.trainable = False"]},{"cell_type":"markdown","metadata":{"id":"qji21sMPuNM7"},"source":["**Fine-tuning the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCDTeH9RuNM7"},"outputs":[],"source":["model.compile(loss=\"binary_crossentropy\",\n","              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n","              metrics=[\"accuracy\"])\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"fine_tuning.keras\",\n","        save_best_only=True,\n","        monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_dataset,\n","    epochs=30,\n","    validation_data=validation_dataset,\n","    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnDB8mICuNM7"},"outputs":[],"source":["model = keras.models.load_model(\"fine_tuning.keras\")\n","test_loss, test_acc = model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"XjU-0O-GuNM8"},"source":["## Summary"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}